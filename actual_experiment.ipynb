{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports, data loading, & data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "include(\"EVGONN.jl\")\n",
    "include(\"NN.jl\")\n",
    "using .EVGONN\n",
    "using .NN\n",
    "using StatsBase\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 10;\n",
    "n_var = 784;\n",
    "n = 60000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = FashionMNIST.traindata()\n",
    "test_x, test_y  = FashionMNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reshape(convert(Array{Float64}, train_x), (n_var, n));\n",
    "X_test = reshape(convert(Array{Float64}, test_x), (n_var, 10000));\n",
    "X = hcat(X_train, X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = fit(ZScoreTransform, Array(X), dims=1)\n",
    "X = StatsBase.transform(dt, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reshape(X[:,1:60000], (n_var, 1, 60000));\n",
    "X_test = reshape(X[:,60001:70000], (n_var, 1, 10000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.+ 1;\n",
    "y_test = test_y.+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = zeros(UInt8, (n, n_categories))\n",
    "for i in 1:n\n",
    "    y_train[i, y[i]] = 0x01\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize each separately and see overall which can attain better accuracy\n",
    "# explore # of hidden layers, # nodes, and experiment with changing learning rate over time\n",
    "# also compare # of iterations and running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_van = NN.NeuralNetwork(n_var, (35,20,20), n_categories, η=0.01);\n",
    "nn_evgo = EVGONN.NeuralNetwork(n_var, (35,20,20), n_categories, 3, η=0.01, β1=0.01, β2=0.000000001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training (generic function with 2 methods)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function training(iters=10000)\n",
    "    costs = zeros(n);\n",
    "    lr_up(x) = min(0.00000000005x^2 + 0.001, 0.51)\n",
    "    nn_van.learning_rate = 0.005\n",
    "    for it in 1:iters\n",
    "        for i in 1:n\n",
    "            result = NN.train(NN.prepare(X_train[:, 1, i]'), NN.prepare(y_train[i, :]'), nn_van)\n",
    "            costs[i] = result[\"cost\"]\n",
    "        end\n",
    "        nn_van.learning_rate = lr_up(it)\n",
    "        if it % 10 == 0\n",
    "            println(\"error = \", sum(costs))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error = 196.1046773957724\n",
      "error = 195.9388902765378\n",
      "error = 195.8657693201574\n",
      "error = 195.80151496103454\n",
      "error = 195.7417236667833\n",
      "error = 195.684628314463\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] Array at ./boot.jl:408 [inlined]",
      " [2] Array at ./boot.jl:416 [inlined]",
      " [3] similar at ./array.jl:380 [inlined]",
      " [4] *(::Array{Float64,2}, ::Array{Float64,2}) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:160",
      " [5] * at ./operators.jl:538 [inlined]",
      " [6] backpropagate(::Array{Float64,2}, ::Main.NN.NeuralNetwork) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/NN.jl:95",
      " [7] train(::Array{Float64,2}, ::Array{Float64,2}, ::Main.NN.NeuralNetwork) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/NN.jl:118",
      " [8] training(::Int64) at ./In[39]:7",
      " [9] training() at ./In[39]:2",
      " [10] top-level scope at In[40]:1",
      " [11] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [12] execute_code(::String, ::String) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [13] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [14] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [15] invokelatest at ./essentials.jl:709 [inlined]",
      " [16] eventloop(::ZMQ.Socket) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [17] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check()\n",
    "    checks = Dict(\"HIT\" => 0, \"MISS\" => 0)\n",
    "    for i in 1:10000\n",
    "        if argmax(NN.predict(NN.prepare(X_test[:, 1, i]'), nn_van)[\"result\"])[2] == y_test[i]\n",
    "            checks[\"HIT\"] += 1\n",
    "        else\n",
    "            checks[\"MISS\"] += 1\n",
    "        end\n",
    "    end\n",
    "    println(\"accuracy = $(checks[\"HIT\"] / 10000)%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8181%\n",
      "Dict(\"MISS\" => 1819,\"HIT\" => 8181)\n"
     ]
    }
   ],
   "source": [
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training (generic function with 2 methods)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function training(iters=10000)\n",
    "    costs = zeros(n);\n",
    "    lr_up(x) = min(0.0000000005x^2 + 0.01, 0.51)\n",
    "    for it in 1:iters\n",
    "        for i in 1:10000\n",
    "            result = EVGONN.train(EVGONN.prepare(X_train[:, 1, i]'), EVGONN.prepare(y_train[i, :]'), nn_evgo)\n",
    "            costs[i] = result[\"cost\"]\n",
    "        end\n",
    "        nn_evgo.learning_rate = lr_up(it)\n",
    "        if it % 10 == 0\n",
    "            println(\"error = \", sum(costs))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error = 1262.1552947550217\n",
      "error = 997.4285736954389\n",
      "error = 854.5976284823194\n",
      "error = 753.6943092255812\n",
      "error = 673.63496370121\n",
      "error = 611.060478253066\n",
      "error = 560.1007065957589\n",
      "error = 518.5787929332012\n",
      "error = 484.9428160282109\n",
      "error = 456.3839067719049\n",
      "error = 430.4203315469887\n",
      "error = 408.6806584804991\n",
      "error = 389.90349422547985\n",
      "error = 369.0968417824938\n",
      "error = 352.6477441028264\n",
      "error = 336.16916330380167\n",
      "error = 322.3939059980115\n",
      "error = 309.1771216248722\n",
      "error = 296.81799656608564\n",
      "error = 286.0898702774582\n",
      "error = 275.4978285292293\n",
      "error = 265.1705972479675\n",
      "error = 255.67502678655507\n",
      "error = 247.67382371570085\n",
      "error = 239.75218154174536\n",
      "error = 232.39579034659891\n",
      "error = 226.0713113839563\n",
      "error = 220.00899087710687\n",
      "error = 214.56129959380343\n",
      "error = 209.10180138657063\n",
      "error = 204.08240333775817\n",
      "error = 198.9590958464039\n",
      "error = 194.27170559700895\n",
      "error = 190.66304527038366\n",
      "error = 186.6724597946394\n",
      "error = 183.21430377555205\n",
      "error = 179.82502612685644\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] backpropagate(::Array{Float64,2}, ::Array{Float64,2}, ::Main.EVGONN.NeuralNetwork, ::Int64, ::Bool) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/EVGONN.jl:106",
      " [2] train(::Array{Float64,2}, ::Array{Float64,2}, ::Main.EVGONN.NeuralNetwork) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/EVGONN.jl:159",
      " [3] training(::Int64) at ./In[88]:6",
      " [4] training() at ./In[88]:2",
      " [5] top-level scope at In[89]:1",
      " [6] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [7] execute_code(::String, ::String) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [8] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [9] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [10] invokelatest at ./essentials.jl:709 [inlined]",
      " [11] eventloop(::ZMQ.Socket) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [12] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check()\n",
    "    checks = Dict(\"HIT\" => 0, \"MISS\" => 0)\n",
    "    for i in 1:10000\n",
    "        if argmax(EVGONN.predict(EVGONN.prepare(X_test[:, 1, i]'), nn_evgo)[\"result\"])[2] == y_test[i]\n",
    "            checks[\"HIT\"] += 1\n",
    "        else\n",
    "            checks[\"MISS\"] += 1\n",
    "        end\n",
    "    end\n",
    "    println(\"accuracy = $(checks[\"HIT\"] / 10000)%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7895%\n"
     ]
    }
   ],
   "source": [
    "check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically training is fucking slow for both (had to train on only 10k to even make it reasonable) but evgo does it in way less epochs. for evgo loss decreases more consistently but each epoch is longer. which makes sense. it's also less sensitive to initialization obviously. and i didn't have to babysit the learning rate. honestly the convergence was pretty sexy just have to be patient, regular gd was either really slow or too bouncy in the 190s but evgo handled it really well. i think it's overfitting though, probably need to run on full 60k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evgo nns only, run a grid search over η, β1, and β2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
