{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports, data loading, & data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "include(\"EVGONN.jl\")\n",
    "include(\"NN.jl\")\n",
    "using .EVGONN\n",
    "using .NN\n",
    "using StatsBase\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 10;\n",
    "n_var = 784;\n",
    "n = 60000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = FashionMNIST.traindata()\n",
    "test_x, test_y  = FashionMNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reshape(convert(Array{Float64}, train_x), (n_var, n));\n",
    "X_test = reshape(convert(Array{Float64}, test_x), (n_var, 10000));\n",
    "X = hcat(X_train, X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = fit(ZScoreTransform, Array(X), dims=1)\n",
    "X = StatsBase.transform(dt, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reshape(X[:,1:60000], (n_var, 1, 60000));\n",
    "X_test = reshape(X[:,60001:70000], (n_var, 1, 10000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.+ 1;\n",
    "y_test = test_y.+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = zeros(UInt8, (n, n_categories))\n",
    "for i in 1:n_train\n",
    "    y_train[i, train_y[i]] = 0x01\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize each separately and see overall which can attain better accuracy\n",
    "# explore # of hidden layers, # nodes, and experiment with changing learning rate over time\n",
    "# also compare # of iterations and running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_van = NN.NeuralNetwork(n_var, (40,20,20), n_categories, η=0.01);\n",
    "nn_evgo = EVGONN.NeuralNetwork(n_var, (40,20,20), n_categories, 3, η=0.01, β1=0.01, β2=0.0000001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training (generic function with 2 methods)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function training(iters=10000)\n",
    "    costs = zeros(n);\n",
    "    lr_up(x) = min(0.00000000005x^2 + 0.001, 0.51)\n",
    "    nn_van.learning_rate = 0.005\n",
    "    for it in 1:iters\n",
    "        for i in 1:n\n",
    "            result = NN.train(NN.prepare(X_train[:, 1, i]'), NN.prepare(y_train[i, :]'), nn_van)\n",
    "            costs[i] = result[\"cost\"]\n",
    "        end\n",
    "        nn_van.learning_rate = lr_up(it)\n",
    "        if it % 10 == 0\n",
    "            println(\"error = \", sum(costs))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error = 196.1046773957724\n",
      "error = 195.9388902765378\n",
      "error = 195.8657693201574\n",
      "error = 195.80151496103454\n",
      "error = 195.7417236667833\n",
      "error = 195.684628314463\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] Array at ./boot.jl:408 [inlined]",
      " [2] Array at ./boot.jl:416 [inlined]",
      " [3] similar at ./array.jl:380 [inlined]",
      " [4] *(::Array{Float64,2}, ::Array{Float64,2}) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:160",
      " [5] * at ./operators.jl:538 [inlined]",
      " [6] backpropagate(::Array{Float64,2}, ::Main.NN.NeuralNetwork) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/NN.jl:95",
      " [7] train(::Array{Float64,2}, ::Array{Float64,2}, ::Main.NN.NeuralNetwork) at /Users/markellekelly/Desktop/CS268/cs268_final_proj/NN.jl:118",
      " [8] training(::Int64) at ./In[39]:7",
      " [9] training() at ./In[39]:2",
      " [10] top-level scope at In[40]:1",
      " [11] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [12] execute_code(::String, ::String) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [13] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [14] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [15] invokelatest at ./essentials.jl:709 [inlined]",
      " [16] eventloop(::ZMQ.Socket) at /Users/markellekelly/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [17] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check()\n",
    "    checks = Dict(\"HIT\" => 0, \"MISS\" => 0)\n",
    "    for i in 1:10000\n",
    "        if argmax(NN.predict(NN.prepare(X_test[:, 1, i]'), nn_van)[\"result\"])[2] == y_test[i]\n",
    "            checks[\"HIT\"] += 1\n",
    "        else\n",
    "            checks[\"MISS\"] += 1\n",
    "        end\n",
    "    end\n",
    "    println(\"accuracy = $(checks[\"HIT\"] / 10000)%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8181%\n",
      "Dict(\"MISS\" => 1819,\"HIT\" => 8181)\n"
     ]
    }
   ],
   "source": [
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check(nn)\n",
    "    checks = Dict(\"HIT\" => 0, \"MISS\" => 0)\n",
    "    for i in 1:10000\n",
    "        if argmax(EVGONN.predict(EVGONN.prepare(X_test[:, 1, i]'), nn)[\"result\"])[2] == y_test[i]\n",
    "            checks[\"HIT\"] += 1\n",
    "        else\n",
    "            checks[\"MISS\"] += 1\n",
    "        end\n",
    "    end\n",
    "    return checks[\"HIT\"] / 10000\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training (generic function with 2 methods)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function training(iters=10000)\n",
    "    costs = zeros(n)\n",
    "    old_costs = sum(costs)\n",
    "    #lr_up(x) = min(0.0000000005x^2 + 0.01, 0.51)\n",
    "    for it in 1:iters\n",
    "        for i in 1:n\n",
    "            result = EVGONN.train(EVGONN.prepare(X_train[:, 1, i]'), EVGONN.prepare(y_train[i, :]'), nn_evgo)\n",
    "            costs[i] = result[\"cost\"]\n",
    "        end\n",
    "        #nn_evgo.learning_rate = lr_up(it)\n",
    "        if it % 10 == 0\n",
    "            println(\"error = \", sum(costs))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error = 14109.02550399244\n",
      "error = 12395.39323745981\n",
      "error = 11638.482452913066\n",
      "error = 10887.420602344482\n",
      "error = 10372.743525521475\n",
      "error = 9842.5310931854\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically training is fucking slow for both (had to train on only 10k to even make it reasonable) but evgo does it in way less epochs. for evgo loss decreases more consistently but each epoch is longer. which makes sense. it's also less sensitive to initialization obviously. and i didn't have to babysit the learning rate. honestly the convergence was pretty sexy just have to be patient, regular gd was either really slow or too bouncy in the 190s but evgo handled it really well. i think it's overfitting though, probably need to run on full 60k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
