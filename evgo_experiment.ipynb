{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVGONeuralNetwork:\n",
    "    \n",
    "    def __init__(self, nlayers, nnodes, activations, t):\n",
    "        assert nlayers == len(nnodes) - 2\n",
    "        assert nlayers == len(activations) - 2\n",
    "        self.nlayers = nlayers\n",
    "        self.nnodes = nnodes\n",
    "        self.activations = activations\n",
    "        self.weights = self.initialize_weights()\n",
    "        self.best_weights = self.weights[0]\n",
    "        self.z = []\n",
    "        self.h = []\n",
    "        self.t = t\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        all_weights = []\n",
    "        for k in range(self.t):\n",
    "            weights = []\n",
    "            for i in range(self.nlayers+1):\n",
    "                weights_i = []\n",
    "                for j in range((self.nnodes[i]+1)*self.nnodes[i+1]):\n",
    "                    weights_i.append(np.random.uniform(-0.25,0.25))\n",
    "                weights.append(weights_i)\n",
    "            all_weights.append(weights)\n",
    "        return all_weights\n",
    "    \n",
    "    def chunker(self, seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "    def forward_prop(self, input_data):\n",
    "        assert len(input_data[0]) == self.nnodes[0]\n",
    "        self.z=[]\n",
    "        inputs=[0]\n",
    "        z=[]\n",
    "        inputs[0] = np.repeat(1,len(input_data))\n",
    "        for i in range(len(input_data[0])):\n",
    "            add = np.array([j[i] for j in input_data])\n",
    "            inputs.append(add)\n",
    "            z.append(add)\n",
    "        self.z.append(z)\n",
    "        self.h = self.z.copy()\n",
    "        for i in range(self.nlayers+1):\n",
    "            new_nodes = [0] * self.nnodes[i+1]\n",
    "            p = 0\n",
    "            #go through the weights for each input node one group at a time \n",
    "            for w in self.chunker(self.best_weights[i], len(new_nodes)):\n",
    "                for j in range(len(w)):\n",
    "                    # add the value corresponding to the jth node in the next layer\n",
    "                    new_nodes[j]= new_nodes[j]+ w[j]*inputs[p]\n",
    "                p += 1\n",
    "            self.z.append(new_nodes)\n",
    "            self.h.append([self.activations[i+1](j) for j in new_nodes])\n",
    "            inputs = self.h[i+1].copy()\n",
    "            inputs.insert(0,np.repeat(1,len(inputs[0])))\n",
    "        return self.z[self.nlayers+1]\n",
    "    \n",
    "    def back_prop(self, y_pred, y, rate, derivs):\n",
    "        deltas = y_pred - y[0]\n",
    "        new_weights = []\n",
    "        for layer in range(self.nlayers,-1,-1):\n",
    "            i=0\n",
    "            new_w = []\n",
    "            n = len(self.h[layer][0])\n",
    "            h_vals = self.h[layer].copy()\n",
    "            h_vals.insert(0,np.repeat(1,n))\n",
    "            for h in h_vals:\n",
    "                for d in deltas:\n",
    "                    #sum weight changes across observations\n",
    "                    changes = np.sum(h*d)\n",
    "                    old_w = self.weights[layer][i]\n",
    "                    new_w.append(old_w - rate*changes)\n",
    "                    i+=1\n",
    "            new_weights.insert(0,new_w)\n",
    "            i=len(self.z[layer+1])\n",
    "            new_deltas = []\n",
    "            for z in self.z[layer]:\n",
    "                new_d=0\n",
    "                for d in deltas:\n",
    "                    new_d += derivs[layer](z)*d*self.weights[layer][i]\n",
    "                    i+=1\n",
    "                new_deltas.append(new_d)\n",
    "            deltas = new_deltas\n",
    "        self.weights = new_weights\n",
    "                \n",
    "    def gradient_descent(self, data, y_val, rate, batch_size, derivs, tol):\n",
    "        diff = 100\n",
    "        new_MSE = 0\n",
    "        while diff > tol:\n",
    "            old_MSE = new_MSE\n",
    "            new_MSE = 0\n",
    "            prev = 0\n",
    "            while prev < len(data):\n",
    "                nxt = prev+batch_size\n",
    "                if nxt > len(data):\n",
    "                    nxt = len(data)\n",
    "                xs = data.values[prev:nxt]\n",
    "                ys = y_val.values[prev:nxt]\n",
    "                prev = nxt\n",
    "                y_pred = self.forward_prop(xs)\n",
    "                self.back_prop(y_pred,ys,rate,derivs)\n",
    "                new_MSE += np.sum((self.forward_prop(xs)[0]-ys[0])**2)\n",
    "            new_MSE = new_MSE/len(data)\n",
    "            diff = abs(old_MSE - new_MSE)\n",
    "            #print(\"MSE = \"+str(new_MSE))\n",
    "        return new_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
